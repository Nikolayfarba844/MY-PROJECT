{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import attrgetter\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import boxcox \n",
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем данные (без скачивания файлов)\n",
    "\n",
    "base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
    "public_key = ['https://disk.yandex.ru/d/FUi5uSd6BfG_ig', \n",
    "              'https://disk.yandex.ru/d/t9Li4JOfgxuUrg',\n",
    "              'https://disk.yandex.ru/d/Gbt-yAcQrOe3Pw']                # ссылки на данные\n",
    "\n",
    "file_names = ['olist_customers_dataset.csv', \n",
    "             'olist_orders_dataset.csv',\n",
    "             'olist_order_items_dataset.csv']                           # имена файлов\n",
    "\n",
    "datasets = ['customers', 'orders', 'order_items']\n",
    "\n",
    "for i in range(3):\n",
    "    # получаем загрузочную ссылку\n",
    "    final_url = base_url + urlencode(dict(public_key=public_key[i]))\n",
    "    response = requests.get(final_url)\n",
    "    download_url = response.json()['href']\n",
    "    \n",
    "    # загружаем данные по ссылкам\n",
    "    globals()[datasets[i]] = pd.read_csv(download_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Число строк в customers:  ', customers.shape[0])\n",
    "print('Число строк в orders:     ', orders.shape[0])\n",
    "print('Число строк в order_items:', order_items.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# список колонок со временем в orders\n",
    "time_col_ls = ['order_purchase_timestamp', \n",
    "          'order_approved_at', \n",
    "          'order_delivered_carrier_date', \n",
    "          'order_delivered_customer_date', \n",
    "          'order_estimated_delivery_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# переводим временные колонки orders в тип дат\n",
    "orders[time_col_ls] = orders[time_col_ls].apply(lambda x: pd.to_datetime(x, format='%Y-%m-%d'))\n",
    "\n",
    "# переводим временную колонку order_items в тип дат\n",
    "order_items['shipping_limit_date'] = pd.to_datetime(order_items['shipping_limit_date'], format='%Y-%m-%d')\n",
    "orders.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Что считать покупкой?\n",
    "Первое, что приходит в голову - посмотреть order_approved_at (время подтверждения оплаты заказа). Оплатил - значит купил.\n",
    "Проверим сколько записей в этой колонке отсутствует.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.order_approved_at.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отберём только те, где order_approved_at с пропусками и посмотрим на значения order_status\n",
    "(\n",
    "    orders[orders.order_approved_at.isna()]\n",
    "        .order_status\n",
    "        .value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'canceled' - здесь всё понятно, заказы с этим статусом нельзя считать покупками.\n",
    "'created' - созданные, но, по всей видимости, ещё не оплаченные, а значит они ещё не куплены (и будут ли оплачены неизвестно).\n",
    "'delivered' - доставленные пользователю, мне понимается это, как факт получения товара покупателем. Если нет даты подтверждения оплаты, возможно, имеет место какой-то сбой в системе.\n",
    "Взглянем на детали заказа доставленных, но с пропусками даты оплаты.\n",
    "\n",
    "Найдём список order_id из orders со статусом delivered и отфильтруем по нему order_items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отфильтруем orders по пропускам в order_approved_at и order_status == \"delivered\"\n",
    "delivered_not_approved = (\n",
    "    orders\n",
    "        .query('order_approved_at.isna() and order_status == \"delivered\"')\n",
    ")\n",
    "delivered_not_approved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отфильтруем order_items по значениям order_id из предыдущего шага\n",
    "delivered_not_approved_items = (\n",
    "    order_items\n",
    "        .query('order_id in @delivered_not_approved.order_id')\n",
    ")\n",
    "delivered_not_approved_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сравним количество строк отфильтрованных датафрэймов\n",
    "print('Число строк в delivered_not_approved:      ', delivered_not_approved.shape[0])\n",
    "print('Число строк в delivered_not_approved_items:', delivered_not_approved_items.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# найдём повторы order_id\n",
    "order_id_duplicated = (\n",
    "    delivered_not_approved_items\n",
    "        .loc[\n",
    "            delivered_not_approved_items\n",
    "                .duplicated(subset='order_id', keep=False)\n",
    "        ]\n",
    ")\n",
    "order_id_duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Совпадает всё, кроме order_item_id (идентификатор товара внутри одного заказа). Видимо это 2 одинаковых товара в одном заказе.\n",
    "Попутно больше узнали о датасете olist_order_items_dataset.csv - скорее всего в нём содержатся записи по каждой единице товара разными строками, одинаковые в том числе.\n",
    "\n",
    "Выясним, есть ли пропуски в order_status датафрэйма orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    orders\n",
    "        .order_status\n",
    "        .isna()\n",
    "        .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Какой статус имеют заказы c подтверждением оплаты?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отберём только те, где order_approved_at без пропусков и посмотрим на значения order_status\n",
    "(\n",
    "    orders[orders.order_approved_at.isna() == False]\n",
    "        .order_status\n",
    "        .value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Получается все, кроме 'created'.\n",
    "\n",
    "'approved' — подвтверждён и оплачен — покупка;\n",
    "'invoiced' — выставлен счёт и оплачен — покупка;\n",
    "'processing' — оплачен и собирается — покупка;\n",
    "'shipped' — оплачен и отгружен — покупка;\n",
    "'delivered' — оплачен и доставлен - покупка;\n",
    "'unavailable' — недоступен — скорее всего выполнить заказ не удастся;\n",
    "'canceled' — отменён — нельзя считать покупкой.\n",
    "\n",
    "В итоге, покупкой будем считать все статусы, кроме:\n",
    "\n",
    "'created' (встречается только в заказах с пропусками оплаты),\n",
    "'unavailable' (скорее всего выполнить не удастся),\n",
    "'canceled' (отменён).\n",
    "Значения в колонке order_approved_at учитывать не будем (учтено при выборе допустимых статусов).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# список статусов, которые не будем считать покупками\n",
    "no_purchases_statuses = ['created', 'unavailable', 'canceled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Сколько у нас пользователей, которые совершили покупку только один раз?\n",
    "Теперь взглянем на датафрэйм customers. Хочется понять связь между customer_id (позаказный идентификатор пользователя) и customer_unique_id (уникальный идентификатор пользователя), чтобы решить по какой колонке смотреть покупки пользователей.\n",
    "Сравним количество уникальных customer_id и order_id в customers и orders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('orders: количество уникальных значений customer_id:   ', orders.customer_id.nunique())\n",
    "print('orders: количество уникальных значений order_id:      ', orders.order_id.nunique())\n",
    "print('orders: общее количество строк:                       ', orders.shape[0])\n",
    "print('customers: количество уникальных значений customer_id:', customers.customer_id.nunique())\n",
    "print('customers: общее количество строк:                    ', customers.shape[0])\n",
    "print('customers: количество уникальных customer_unique_id:  ', customers.customer_unique_id.nunique())\n",
    "print('customers: количество пропусков  customer_unique_id:  ', customers.customer_unique_id.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Все customer_id в customers и orders уникальны, и им соответствуют уникальные order_id. По customer_id можно объединить эти датафрэймы. Если количество строк inner join при этом сохранится, значит значения customer_id в этих датафрэймах полностью совпадают.\n",
    "\n",
    "Похоже на то, что на часть customer_unique_id приходится по несколько customer_id. Убедимся в этом визуально, показав дубликаты customer_unique_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    customers\n",
    "        .loc[customers\n",
    "                .duplicated(keep=False, subset='customer_unique_id')]\n",
    "        .sort_values('customer_unique_id')\n",
    "        .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Следовательно при оценке покупок необходимо ориентироваться на customer_unique_id. Для этого объединим датасеты customers и orders по колонке customer_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_orders_merged = customers.merge(orders, on='customer_id')\n",
    "customers_orders_merged.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Количество строк у объединённого датафрэйма осталось таким же - значит все значения customer_id в обоих датафрэймах полностью совпадают и никакие значения при объединении не потеряны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_orders_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Число пользователей, совершивших покупку только один раз:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    customers_orders_merged\n",
    "        .query('order_status not in @no_purchases_statuses')  # убираем строки со статусами-непокупками\n",
    "        .groupby('customer_unique_id', as_index=False)        # группируем по customer_unique_id\n",
    "        .agg({'customer_id': 'count'})                        # подсчитываем количество покупок на каждого уник. пользователя\n",
    "        .query('customer_id == 1')                            # отбираем тех, у кого количество покупок = 1\n",
    "        .shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Какой заказ считать недоставленным?\n",
    "Здесь напрашиваются два варианта:\n",
    "\n",
    "Всё, что имеет статус отличный от delivered;\n",
    "Отсутствует дата доставки.\n",
    "Проверим как согласуются колонки со статусом и временем доставки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# уникальные значения статуса среди непропущенных \n",
    "# значений order_delivered_customer_date в orders\n",
    "(\n",
    "    orders\n",
    "        .query('order_delivered_customer_date.isna() == False')\n",
    "        .order_status\n",
    "        .value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отменённые заказы с датой доставки клиенту\n",
    "(\n",
    "    orders\n",
    "        .query('order_delivered_customer_date.isna() == False and order_status == \"canceled\"')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# значения order_status при пропущенных датах доставки в orders\n",
    "(\n",
    "    orders\n",
    "        .query('order_delivered_customer_date.isna() == True')\n",
    "        .order_status\n",
    "        .value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orders с пропущенными датами достаки, при этом имеющие статус \"доставлен\"\n",
    "(\n",
    "    orders\n",
    "        .query('order_delivered_customer_date.isna() == True and order_status == \"delivered\"')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Здесь не наблюдается каких-то закономерностей, в силу неизвестных нам причин заказы одновременно имеют статус delivered и пропущенные значения order_delivered_customer_date.\n",
    "Остановимся на версии №1: всё, что имеет статус отличный от delivered будем считать недоставленным.\n",
    "\n",
    "2. Сколько заказов в месяц в среднем не доставляется по разным причинам?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# датафрэйм с не доставленными заказами\n",
    "not_delivered_orders = (\n",
    "    orders\n",
    "        .copy()\n",
    "        .query('order_status != \"delivered\"')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# статусы недоставленных и их количество\n",
    "(\n",
    "    not_delivered_orders\n",
    "        .order_status\n",
    "        .value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавим колонку содержащую только год и месяц на основе order_estimated_delivery_date\n",
    "not_delivered_orders['year_month_estimated'] = (\n",
    "    not_delivered_orders\n",
    "        .order_estimated_delivery_date\n",
    "        .dt\n",
    "        .strftime(\"%Y-%m\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сгруппируем по колонке \"year_month_estimated\" и статусу заказа, чтобы посчитать количество недоставленных\n",
    "undelivered_by_month = (\n",
    "    not_delivered_orders\n",
    "        .groupby(['year_month_estimated', 'order_status'], as_index=False)\n",
    "        .agg({'order_id': 'count'})\n",
    "        .rename(columns={'order_id': 'undelivered'})\n",
    ")\n",
    "undelivered_by_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# среднее количество недоставленных заказов в месяц\n",
    "undelivered_by_month.undelivered.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Посчитаем среднее количество недоставленных заказов в месяц по каждму статусу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# среднее количество недоставленных заказов по месяцам\n",
    "mean_by_month_df = pd.DataFrame()\n",
    "\n",
    "for status in undelivered_by_month \\\n",
    "                                .order_status \\\n",
    "                                .unique():\n",
    "    temp_df = pd.DataFrame(                                                          \n",
    "        [[status, \n",
    "          undelivered_by_month\n",
    "                  .query('order_status == @status')\n",
    "                  .undelivered\n",
    "                  .mean()]], \n",
    "                columns=['order_status', 'mean_by_month']\n",
    "                          )\n",
    "    mean_by_month_df = pd.concat([mean_by_month_df, temp_df])\n",
    "\n",
    "mean_by_month_df.sort_values('mean_by_month', inplace=True, ascending=False)\n",
    "mean_by_month_df = mean_by_month_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Среднее количество недоставленных заказов в месяц по каждому статусу:\n",
    "mean_by_month_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# недоставленные со статусом shipped\n",
    "shipped = (\n",
    "    not_delivered_orders\n",
    "        .copy()\n",
    "        .query('order_status == \"shipped\"')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разница во времени обещаной датой доставки и датой передачи в логистическую службу\n",
    "shipped['delta_time'] = (\n",
    "    shipped\n",
    "        .order_estimated_delivery_date\n",
    "            .sub(shipped\n",
    "                 .order_delivered_carrier_date)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# недоставленные shipped с отрицательной разницей во времени\n",
    "shipped.query('delta_time < @pd.to_timedelta(0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3. По каждому товару определить, в какой день недели товар чаще всего покупается.\n",
    "Подробнее взглянем на order_items\n",
    "\n",
    "Сравнение датафрэймов orders и order_items\n",
    "Посмотрим на количество уникальных order_id в orders и order_items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('orders: количество уникальных значений order_id:       ', orders.order_id.nunique())\n",
    "print('orders: общее количество строк:                        ', orders.shape[0])\n",
    "print('order_items: количество уникальных значений order_id:  ', order_items.order_id.nunique())\n",
    "print('order_items: общее количество строк:                  ', order_items.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Общее количество строк order_items больше, чем в orders (как мы уже выяснили, на каждый order_id может приходиться несколько записей товаров), а уникальных значений order_id - меньше.\n",
    "Взлянем на order_id, отсутствующие в order_items, но присутствующие в orders:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# список order_id, которых нет в order_items\n",
    "droped_order_id_ls = orders.query('order_id not in @order_items.order_id.unique()').order_id\n",
    "\n",
    "# строки с order_id по которым нет данных в order_items\n",
    "droped_order_id = orders.query('order_id in @droped_order_id_ls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество пропущеных order_id\n",
    "droped_order_id.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "droped_order_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попытаемся найти закономерности\n",
    "\n",
    "# уникальные значения order_status в строках с пропущенными order_id и их количество\n",
    "droped_order_id.order_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# общее количетсво строк с order_status == \"unavailable\" в orders\n",
    "orders.query('order_status == \"unavailable\"').shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# общее количетсво строк с order_status == \"canceled\" в orders\n",
    "orders.query('order_status == \"canceled\"').shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# строки droped_order_id с order_status \"created\", \"invoiced\" или \"shipped\"\n",
    "(\n",
    "    droped_order_id\n",
    "        .query('order_status in [\"created\", \"invoiced\", \"shipped\"]')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пропуски в колонках со временем droped_order_id\n",
    "droped_order_id[time_col_ls].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Все значения order_delivered_customer_date (время доставки заказа) пропущены и во всех значениях order_status нет:\n",
    "\n",
    "delivered (доставлен пользователю),\n",
    "approved (подтверждён),\n",
    "processing (в процессе сборки заказа)\n",
    "Большинство из них либо unavailable, либо canceled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# всего пропущено order_delivered_customer_date в orders\n",
    "orders.order_delivered_customer_date.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# всего пропусков order_estimated_delivery_date в orders\n",
    "orders.order_estimated_delivery_date.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# всего пропусков order_purchase_timestamp в orders\n",
    "orders.order_purchase_timestamp.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Самая ранняя дата order_estimated_delivery_date в orders:          ', orders.order_estimated_delivery_date.min())\n",
    "print('Самая ранняя дата order_estimated_delivery_date в droped_order_id: ', droped_order_id.order_estimated_delivery_date.min())\n",
    "print('Самая поздняя дата order_estimated_delivery_date в orders:         ',orders.order_estimated_delivery_date.max())\n",
    "print('Самая поздняя дата order_estimated_delivery_date в droped_order_id:', droped_order_id.order_estimated_delivery_date.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order_id некупленных товаров\n",
    "no_purchases_order_id = (\n",
    "    orders\n",
    "        .query('order_status == @no_purchases_statuses')\n",
    "    .order_id\n",
    ")\n",
    "\n",
    "# количетсво строк с заказами в order_items, которые мы не считаем покупками\n",
    "(\n",
    "    order_items\n",
    "        .query('order_id in @no_purchases_order_id')\n",
    "        .shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Итак, всё, что можно сказать о отсутствующих строках в order_items - это часть недоставленных товаров, большая из которых - недоступна или отменена. Почти все недоступные не попали в order_items..\n",
    "Также в в order_items имеются заказы, которые мы не считаем покупками.\n",
    "\n",
    "Объединим все датафрэймы в один"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = customers_orders_merged.merge(order_items, on='order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Отберём те строки, что считаются покупками\n",
    "\n",
    "purchase_total_df = (\n",
    "    total_df\n",
    "        .copy()\n",
    "        .query('order_status not in @no_purchases_statuses')\n",
    ")\n",
    "purchase_total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество строк с пропущенными датами оплаты в purchase_total_df\n",
    "purchase_total_df.order_approved_at.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Добавим колонки дней недели на основе order_purchase_timestamp.\n",
    "\n",
    "# добавляем колонку с днями недели создания заказа\n",
    "purchase_total_df['order_weekday'] = (\n",
    "    purchase_total_df\n",
    "        .copy()\n",
    "        .order_purchase_timestamp\n",
    "        .dt\n",
    "        .day_name()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "За одну покупку определённого товара будем считать его наличие в заказе не зависимо от количества. Т.е. если в заказе, например, 5 одинаковых товаров - это одна покупка, а не 5.\n",
    "\n",
    "Одинаково часто встречающихся дней недели может быть несколько, чтобы это учесть создадим функцию, отбирающую все дни недели, которые встречаются максимально часто и преобразует их в строку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция собирающая максимально часто встречающиеся дни недели в одну строку\n",
    "def get_weekdays(weekdays):\n",
    "    max_vals = weekdays.value_counts().max()\n",
    "    weekday_list = (weekdays \n",
    "            .value_counts()[weekdays.value_counts() == max_vals] \n",
    "            .index \n",
    "            .tolist())\n",
    "    return ', '.join(weekday_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Дни недели, в которые чаще всего покупаются товары.\n",
    "%%time\n",
    "purchase_weekday_by_product = (\n",
    "    purchase_total_df\n",
    "        .groupby(['order_id', 'product_id'], as_index=False)  # сначала избавимся от повторов product_id в заказе: группируем по двум колонкам\n",
    "        .agg({'order_weekday': 'first'})                      # выбираем первое значение, т.к. они все одинаковые\n",
    "        .groupby('product_id', as_index=False)                # затем снова группируем, чтобы схлопнуть в одно значение (оставить уникальные)\n",
    "        .agg({'order_weekday': lambda x: get_weekdays(x)})    # подсчитываем и выбираем значение с максимальным \"счётом\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_weekday_by_product.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Сколько у каждого из пользователей в среднем покупок в неделю (по месяцам)?\n",
    "Внутри месяца может быть не целое количество недель. Например, в ноябре 2021 года 4,28 недели. И внутри метрики это нужно учесть.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Расчитаем и добавим колонку mean_orders_per_week со значениями средним значением покупок в неделю по месяцам для каждого пользователя.\n",
    "\n",
    "mean_per_week_df = (\n",
    "    purchase_total_df\n",
    "        .groupby(['year_month_order', 'customer_unique_id'], as_index=False)             # группируем по году-месяцу и пользователю\n",
    "        .agg({'order_purchase_timestamp': 'count'})                                      # считаем количество (в месяц на пользователя)\n",
    "        .assign(weeks_in_month = lambda x: x.year_month_order.dt.daysinmonth / 7)        # добавляем колонку количество недель в текущем месяце\n",
    "        .rename(columns={'order_purchase_timestamp': 'orders_per_month'})                # переименовываем колонку с подсчётом\n",
    "        .assign(mean_orders_per_week = lambda x: x.orders_per_month / x.weeks_in_month)  # добавляем колонку в которой делим количество заказов в месяц на количество недель в месяце\n",
    "        [['year_month_order', 'customer_unique_id', 'mean_orders_per_week']]             # отберём нужные колонки\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Итак, количество покупок каждого пользоватлея в неделю по месяцам\n",
    "mean_per_week_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Написать функцию на python, позволяющую строить когортный анализ. В период с января по декабрь выявить когорту с самым высоким retention на 3-й месяц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество уникальных пользователей\n",
    "num_customers = (\n",
    "    purchase_total_df\n",
    "        .groupby(['customer_unique_id'])\n",
    "        .order_id\n",
    "        .nunique()\n",
    ")\n",
    "# доля пользователей сделавших более 1 заказа\n",
    "mult_orders_perc = (\n",
    "    np.sum(num_customers > 1) / purchase_total_df\n",
    "        .order_id.nunique()\n",
    ")\n",
    "print(f'{100 * mult_orders_perc:.2f}% пользователей сделали более одного заказа.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для когортного анализа\n",
    "def user_retention(data, customer_id, date, period='M'):\n",
    "    '''\n",
    "    Функция строит retention-матрицу когортного анализа (возвращает датафрэйм)\n",
    "    Требуются библиотеки: \n",
    "        from operator import attrgetter\n",
    "        import pandas as pd\n",
    "    Описание полей:\n",
    "    data        - датафрэйм\n",
    "    customer_id - колонка с id клиентов (str)\n",
    "    order_date  - колонка с датами (datetime)\n",
    "    period      - период разбивки (str, по умолчанию \"M\" - месяц). Все псевдонимы: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timeseries-offset-aliases\n",
    "    '''\n",
    "    data = data.copy()\n",
    "    data[date] = data[date].dt.to_period('M')      # переводим колонку дат в тип \"период\"\n",
    "    \n",
    "    # создаём когорты\n",
    "    users_cohorts = (\n",
    "        data\n",
    "            .groupby(customer_id, as_index=False)  # группируем по пользователям\n",
    "            [date]                                 # выбираем колонку с датами\n",
    "            .min()                                 # отбираем минимальные колонки для каждого пользователя\n",
    "            .rename(columns={date: 'cohort'})      # переименовываем колонку с дамами в когорты\n",
    "    )\n",
    "    \n",
    "    # объединим датасеты\n",
    "    cohorts_df = (\n",
    "        users_cohorts                                           # датасет с когортами\n",
    "            .merge(data[[customer_id, date]], on=customer_id)   # объединяем с основным датасетом по пользователям\n",
    "            .sort_values('cohort')                              # сортируем по когортам\n",
    "            .rename(columns={date: 'target_action_dates'})      # переименовываем колонку с датами целевых действий\n",
    "    )\n",
    "    \n",
    "    # сгруппируем по когортам и месяцам покупок и посчитаем количество покупателей на каждый месяц\n",
    "    cohorts_df = (\n",
    "        cohorts_df\n",
    "            .groupby(['cohort', 'target_action_dates'], as_index=False)\n",
    "            .agg(customer_quantity = (customer_id, 'nunique'))\n",
    "    )\n",
    "    \n",
    "    # считаем разницу между первой и последующими покупками, добавляем в столбец\n",
    "    cohorts_df['period_number'] = (\n",
    "        cohorts_df\n",
    "            .target_action_dates                  \n",
    "            .sub(cohorts_df.cohort)               # вычитаем когорты из времени целевых действий\n",
    "            .apply(attrgetter('n'))               # передаём атрибут номинальной единицы (т.е. будут те же что и в  period=)\n",
    "    )\n",
    "    \n",
    "    # строим сводную таблицу\n",
    "    cohort_pivot = cohorts_df.pivot_table(index='cohort', columns='period_number', values='customer_quantity')\n",
    "    \n",
    "    # размер когорты = количество пользователей в нулевой период\n",
    "    cohort_size = cohort_pivot.iloc[:,0]\n",
    "    \n",
    "    # строим retention матрицу\n",
    "    retention_matrix = cohort_pivot.div(cohort_size, axis = 0)\n",
    "\n",
    "    return retention_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаём retention матрицу используя функцию\n",
    "retention_matrix = user_retention(purchase_total_df, 'customer_unique_id', 'order_purchase_timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# применим форматирование отображения датафрэйма\n",
    "ur_style = (retention_matrix\n",
    "            .style \n",
    "            .set_caption('Customer retention by cohort')  # добавляем подпись \n",
    "            .background_gradient(cmap='viridis')          # раскрашиваем ячейки по столбцам \n",
    "            .highlight_null('white')                      # делаем белый фон для значений NaN \n",
    "            .format(\"{:.2%}\", na_rep=\"\"))                 # числа форматируем как проценты, NaN заменяем на пустоту \n",
    "ur_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# когорта с максимальным retention на третий месяц \n",
    "# в период с января по декабрь 2017 г.\n",
    "print(retention_matrix.loc['2017-01':'2017-12', 3].idxmax())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Построить RFM-кластеры для пользователей. Вывести для каждого кластера средние значения метрик R, F, M .\n",
    "R - recency — давность (как давно ваши клиенты что-то покупали);\n",
    "F - frequency — частота (как часто клиенты покупают);\n",
    "M - monetary — деньги (общая сумма покупок).\n",
    "\n",
    "По каждому из этих признаков мы выделяем по нексолько групп (точное количество определим в процессе). Затем присваиваем каждой группе числовое обозначение от 1 до n, где n - число кластеров\n",
    "\n",
    "За отчётный период берём один год.\n",
    "\n",
    "Нам потребуются следующие данные:\n",
    "\n",
    "id клиента\n",
    "даты покупок\n",
    "общее число заказов в течение указанного отчетного периода\n",
    "средний чек\n",
    "Взлянем на даты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сегодняшняя дата (действительная)\n",
    "real_today = (\n",
    "    pd.to_datetime(\n",
    "        datetime.datetime.today()\n",
    "              )\n",
    ")\n",
    "real_today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# дата первого заказа\n",
    "first_order_date = (\n",
    "    purchase_total_df\n",
    "                 .order_purchase_timestamp\n",
    "                 .min()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# дата последнего заказа\n",
    "last_order_date = (\n",
    "    purchase_total_df\n",
    "                 .order_purchase_timestamp\n",
    "                 .max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разница между сегодняшним днём и датой последнего заказа в месяцах\n",
    "(real_today.to_period('M') - last_order_date.to_period('M')).n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разница между сегодняшним днём и датой первого заказа в месяцах\n",
    "(real_today.to_period('M') - first_order_date.to_period('M')).n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разница между датами последнего и первого заказов в месяцах\n",
    "(last_order_date.to_period('M') - first_order_date.to_period('M')).n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Последняя дата заказа - сентябрь 2018 и новее данных нет. Разница между сегодняшней датои и датой последнего заказа почти в два раза больше разницы между первым и последним заказом. Получаестся, что дата первого заказа не сильно отличается от даты последнего заказа по отношению к сегодняшнему дню. С этим связана некоторая сложность в оценке того, что принять за \"давнюю дату\", а что за \"недавнюю\".\n",
    "Чтобы увидеть более реальную картину относительно давности заказов, будем оценивать даты относительно последнего дня заказа, вместо действительного сегодняшнего дня. Т.е. как бы переместимся в прошлое, чтобы посмотреть как вели себя покупатели тогда."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# условня \"сегодняшняя\" дата\n",
    "conditional_today_date = purchase_total_df.order_purchase_timestamp.max()\n",
    "conditional_today_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# дата начала периода\n",
    "start_date = conditional_today_date - pd.Timedelta(365, unit='day')\n",
    "start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Подготовим датасет для rfm-анализа. Для этого сгруппируем имеющийся датасет с данными о покупках purchase_total_df и найдём последнюю дату покупок, средний чек и количество заказов на пользователя.\n",
    "На общую сумму полученную с покупателя может влиять количество покупок и, как следствие, дублирование метрик. Поэтому в качестве параметра Monetary выбрана средняя сумма чека.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# датафрэём для анализа за весь период времени\n",
    "pre_rfm_all_years = (\n",
    "    purchase_total_df\n",
    "        .copy()\n",
    "\n",
    "    # сначала сгруппируем по заказам\n",
    "        .groupby('order_id', as_index=False)                     \n",
    "        .agg({\n",
    "            'price': 'sum',                               # найдём сумму заказа\n",
    "            'customer_unique_id': 'first',                # сохраним колонку пользователя, взяв первый элемент из группы\n",
    "            'order_purchase_timestamp': 'max'             # оставим последнюю дату\n",
    "        })\n",
    "    \n",
    "    # теперь группируем по пользователям\n",
    "        .groupby('customer_unique_id', as_index=False)           \n",
    "        .agg({\n",
    "            'price': 'mean',                              # средний чек\n",
    "            'order_id': 'count',                          # количество покупок\n",
    "            'order_purchase_timestamp': 'max'})           # последнее время заказа\n",
    "        .rename(columns={\n",
    "            'price': 'avg_bill', \n",
    "            'customer_unique_id': 'customer',\n",
    "            'order_id': 'purchases_quantity',\n",
    "            'order_purchase_timestamp': 'last_order'\n",
    "        })\n",
    "        .assign(day_quantity = lambda x:                  # добавляем колонку с разницей между условным сегодняшним днём и последней покупкой в днях\n",
    "                (\n",
    "                    conditional_today_date.to_period('D') - x.last_order.dt.to_period('D')\n",
    "                )\n",
    "                .apply(attrgetter('n'))                   # передадим созданной колонке атрибут n, чтобы задать номинальные единицы изменения (дни)\n",
    "               )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# датафрэём для анализа за один год\n",
    "pre_rfm = (\n",
    "    purchase_total_df\n",
    "        .copy()\n",
    "        .query('order_purchase_timestamp >= @start_date') \n",
    "    \n",
    "    # сначала сгруппируем по заказам\n",
    "        .groupby('order_id', as_index=False)                     \n",
    "        .agg({\n",
    "            'price': 'sum',                               # найдём сумму заказа\n",
    "            'customer_unique_id': 'first',                # сохраним колонку пользователя, взяв первый элемент из группы\n",
    "            'order_purchase_timestamp': 'max'             # оставим последнюю дату\n",
    "        })\n",
    "    \n",
    "    # теперь группируем по пользователям\n",
    "        .groupby('customer_unique_id', as_index=False)           \n",
    "        .agg({\n",
    "            'price': 'mean',                              # средний чек\n",
    "            'order_id': 'count',                          # количество покупок\n",
    "            'order_purchase_timestamp': 'max'})           # последнее время заказа\n",
    "        .rename(columns={\n",
    "            'price': 'avg_bill', \n",
    "            'customer_unique_id': 'customer',\n",
    "            'order_id': 'purchases_quantity',\n",
    "            'order_purchase_timestamp': 'last_order'\n",
    "        })\n",
    "        .assign(day_quantity = lambda x:                  # добавляем колонку с разницей между условным сегодняшним днём и последней покупкой в днях\n",
    "                (\n",
    "                    conditional_today_date.to_period('D') - x.last_order.dt.to_period('D')\n",
    "                )\n",
    "                .apply(attrgetter('n'))                   # передадим созданной колонке атрибут n, чтобы задать номинальные единицы изменения (дни)\n",
    "               )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Основная сложность проведения анализа — определить границы сегментов. Нам не известно что это за магазин и какими видами товара он торгует. Чтобы понять, что есть норма для этого бизнеса, посмотрим на медианные, средние, минимальные и максимальные значения данных, а также более детально посмотрим на их распределения.\n",
    "\n",
    "pre_rfm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Средний чек: 75-й процентиль 149,9, а максимум - 13440, к тому же среднее и медианное значения отличаются значительно. Среднюю меру здесь описывает медиана 88.34, а высокий максимум обусловлен выбросами - редкими покупками на очень большие суммы.\n",
    "Количество покупок: подавляющее большинство покупателей делают одну покупку, также как и в случае со средним чеком имеют место выбросы.\n",
    "Количество дней с последней покупки: здесь картина более-менее равномерная, среднее и медиана почти равны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monetary\n",
    "# распределение средних чеков покупателя с логарифмической шкалой Y\n",
    "sns.displot(pre_rfm.avg_bill, height=7, aspect=2, log_scale=(False, True))\n",
    "plt.xlabel(\"Average Bill\", size=14)\n",
    "plt.ylabel(\"Count (logarithmic scale)\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# распределение средних чеков покупателя, левая часть более детально \n",
    "sns.displot(pre_rfm.avg_bill[pre_rfm.avg_bill < 500], height=7, aspect=1, kde=True)\n",
    "plt.xlabel(\"Average Bill\", size=14)\n",
    "plt.ylabel(\"Count\", size=14)\n",
    "\n",
    "plt.axvline(x=75,              # синяя пунктирная линия - граница между 1 и 2 группами\n",
    "            color='blue',\n",
    "            ls='--', \n",
    "            lw=2.5)\n",
    "plt.axvline(x=280,             # фиолетовая пунктирная линия - граница между 2 и 3 группами\n",
    "            color='purple',\n",
    "            ls='--', \n",
    "            lw=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Для начала попытаемся определит интервалы, интерпретирую графики. Глядя на рапсределение можно выделить следующие группы: От 0 до значения чуть меньшего медианы - 75.\n",
    "Вторая - свыше 75 до 280.\n",
    "Третья - свыше 280.\n",
    "На границах этих групп заметны значительные перепады, или изменение характера апроксимирующей кривой графика.\n",
    "\n",
    "Далее с попощью метода К-средних определим порги и назначим номера кластеров для каждой из метрик.\n",
    "Следующим шагом определим оптимальное количство кластеров для колонки со средними чеками с помощью метода локтя. Для этого напишем функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_cluster_number(X, n_clucter_max=7):\n",
    "    '''\n",
    "    Функция рисует график для определения оптимального количества кластеров, который строится с момощью метода локтя \n",
    "    X - пандосовская серия\n",
    "    n_clucter_max - максимальное количество кластеров\n",
    "    '''\n",
    "    error_rates = []\n",
    "    for i in range(1, n_clucter_max + 1):\n",
    "        model = KMeans(n_clusters = i, random_state = 42)  # создадим экземпляр класса KMeans\n",
    "        model.fit(X.values.reshape(-1,1))                                      # преобразуем 1D массив в 2D и обучим модель \n",
    "        error_rates.append(model.inertia_)                                     # наполняем список для построения графика\n",
    "\n",
    "    plt.plot(range(1, n_clucter_max + 1), error_rates)\n",
    "    plt.xlabel(\"Number of Cluster\", size=14)\n",
    "    plt.ylabel(\"Errors\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Находим оптимальное количество кластеров для среднего чека. Сначала попробуем привести колонку со средними чеками к нормальному виду, на сколько это возможно, с помощью Box-Cox трансформации, иначе имеющиеся выбросы исказят результат.\n",
    "\n",
    "# трнасформируем средний чек и соединим его с нетрансформированным\n",
    "transformed_data, best_lambda = boxcox(pre_rfm.avg_bill)\n",
    "trans_avg_bill = pd.DataFrame(transformed_data, columns=['trans_bill'])\n",
    "trans_avg_bill['avg_bill'] = pre_rfm.avg_bill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# распределение средних чеков покупателя после Box-Cox трансформации\n",
    "sns.displot(trans_avg_bill.trans_bill, height=7, aspect=1)\n",
    "plt.xlabel(\"Average Bill\", size=14)\n",
    "plt.ylabel(\"Count\", size=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ищем оптимальное количество кластеров\n",
    "optimal_cluster_number(trans_avg_bill.trans_bill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_borders(X, n_clusters):\n",
    "    '''\n",
    "    Функция возвращает список границ кластеров\n",
    "    Принимает на вход:\n",
    "        X - пандосовская серия\n",
    "        n_clusters - количество кластеров\n",
    "    '''\n",
    "    model = KMeans(n_clusters=n_clusters, random_state = 0)     # создадим экземпляр класса KMeans\n",
    "    model.fit(X.values.reshape(-1,1))                           # преобразуем 1D массив в 2D и обучим модель \n",
    "    clusters = model.labels_                                    # кластеры   \n",
    "\n",
    "    df = X.to_frame()                                           # переведём серию в датафрэйм\n",
    "    df['clusters'] = clusters                                   # добавим колонку с кластерами\n",
    "\n",
    "    borders = [df.iloc[:, 0].min()]                             # добавим минимальное значение в список\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        border = df.query('clusters == @i').iloc[:, 0].max()    # отбираем максимальные значения при определённых значениях clusters\n",
    "        borders.append(border)                                  # добавляем в список\n",
    "    \n",
    "    return sorted(borders)                                      # возвращаем сортированный список границ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# границы трансформированного прайса\n",
    "m_borders_trans = get_borders(trans_avg_bill.trans_bill, 3)\n",
    "m_borders_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# переведём границы трансформированного прайса в нетрансформированные\n",
    "m_borders = []\n",
    "for i in m_borders_trans:\n",
    "    m_borders.append(trans_avg_bill[trans_avg_bill.trans_bill == i].avg_bill.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# границы интервалов Monetary\n",
    "m_borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# распределение средних чеков покупателя, с нанесёнными границами (левая часть более детально)\n",
    "sns.displot(pre_rfm.avg_bill[pre_rfm.avg_bill < 500], height=7, aspect=1, kde=True)\n",
    "plt.xlabel(\"Average Bill\", size=14)\n",
    "plt.ylabel(\"Count\", size=14)\n",
    "\n",
    "plt.axvline(x=m_borders[1],              # синяя пунктирная линия - граница между 1 и 2 группами\n",
    "            color='blue',\n",
    "            ls='--', \n",
    "            lw=2.5)\n",
    "plt.axvline(x=m_borders[2],             # фиолетовая пунктирная линия - граница между 2 и 3 группами\n",
    "            color='purple',\n",
    "            ls='--', \n",
    "            lw=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# назначаем рейтинг для Monetary\n",
    "m_labels = (1, 2, 3)\n",
    "m_score_bins = m_borders\n",
    "\n",
    "m_score = (\n",
    "    pd.cut(pre_rfm.avg_bill, \n",
    "                   bins=m_score_bins, \n",
    "                   labels = m_labels, \n",
    "                   right=True,\n",
    "                   include_lowest = True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# доля пользователей, сделавших только один заказ\n",
    "single_order_perc = (\n",
    "    pre_rfm.purchases_quantity.value_counts()[1]\n",
    "    /\n",
    "    pre_rfm.purchases_quantity.count()\n",
    ")\n",
    "print(f'{100 * single_order_perc:.2f}% пользователей сделали только один заказ.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# распределение количества заказов покупателя с логарифмической шкалой Y\n",
    "sns.displot(pre_rfm.purchases_quantity, height=7, aspect=1, log_scale=(False, True))\n",
    "plt.xlabel(\"Number Of Orders per Customer\", size=14)\n",
    "plt.ylabel(\"Count (logarithmic scale)\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# распределение количества заказов на покупателя с количеством покупок <= 3\n",
    "sns.displot(pre_rfm.purchases_quantity[pre_rfm.purchases_quantity <= 3], height=7)\n",
    "plt.xlabel(\"Number Of Orders per Customer\", size=14)\n",
    "plt.ylabel(\"Count\", size=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# распределение количества заказов на покупателя с количеством покупок >= 3\n",
    "sns.displot(pre_rfm.purchases_quantity[pre_rfm.purchases_quantity >= 3], height=7)\n",
    "plt.xlabel(\"Number Of Orders per Customer\", size=14)\n",
    "plt.ylabel(\"Count\", size=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Аналогичным образом, попытаемся определит интервалы интерпретируюя графики.\n",
    "Глядя на цифры и распределения видим, что общую массу покупателей можно разделить на 3 группы по характерным \"ступеням\" - резким перепадам, показывающим своеобразный переход на новый уровень лояльности к магазину.\n",
    "Преобладают покупатели с одним количеством заказов - 71240 (97.49%). Эту часть покупателей определим в группу №1. Число покупателей, сделавших только 2 покупки - 1709 - группа №2. В третью группу возьмём оставшихся с числом покупок 3 и более.\n",
    "Итак:\n",
    "Группа 1 - только один заказ.\n",
    "Группа 2 - только два заказа.\n",
    "Группа 3 - 3 и более заказов.\n",
    "\n",
    "Далее воспользуемся кластеризацией методом k-средних и уточним выбранные границы.\n",
    "Находим оптимальное количество кластеров для количества покупок с помощью k-means и метода локтя. В данносм случае не требуетются какие-либо преобразования распределения.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оптимальное количество кластеров для purchases_quantity\n",
    "optimal_cluster_number(pre_rfm.purchases_quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# интервалы для Frequency\n",
    "f_borders = get_borders(pre_rfm\n",
    "                            .purchases_quantity\n",
    "                        , 3)\n",
    "f_borders[0] = 0                                # присвоим первому (нулевому) элементу значение 0, вместо 1, чтобы cut() мог обработать этот список\n",
    "f_borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# назначаем рейтинг для Frequency\n",
    "f_labels = (1, 2, 3)\n",
    "f_score_bins = f_borders\n",
    "\n",
    "f_score = (\n",
    "    pd.cut(pre_rfm.purchases_quantity, \n",
    "                   bins=f_score_bins, \n",
    "                   labels = f_labels, \n",
    "                   right=True, \n",
    "                   include_lowest=True)\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recency\n",
    "# распределение колонки давности последней покупки\n",
    "sns.displot(pre_rfm.day_quantity, height=7, aspect=1, kde=True)\n",
    "plt.xlabel(\"Days Since Last Purchase per Customer\", size=14)\n",
    "plt.ylabel(\"Count\", size=14)\n",
    "\n",
    "plt.axvline(x=pre_rfm.day_quantity.median(),               # зелёная пунктирная линия - медиана\n",
    "            color='green',\n",
    "            ls='--', \n",
    "            lw=2.5)\n",
    "plt.axvline(x=pre_rfm.day_quantity.quantile(q=0.33),       # синяя пунктирная линия - 33-й процентиль\n",
    "            color='blue',\n",
    "            ls='--', \n",
    "            lw=2.5)\n",
    "plt.axvline(x=pre_rfm.day_quantity.quantile(q=0.66),       # фиолетовая пунктирная линия - 66-й процентиль\n",
    "            color='purple',\n",
    "            ls='--', \n",
    "            lw=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33-й и 66-й процентили количества дней от последней покупки\n",
    "pre_rfm.day_quantity.quantile(q=[0.33, 0.66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оптимальное количество кластеров для day_quantity\n",
    "optimal_cluster_number(pre_rfm.day_quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# границы разбивки для recency\n",
    "r_borders = get_borders(pre_rfm.day_quantity, 3)\n",
    "r_borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# назначаем рейтинг для recency\n",
    "r_labels = (3, 2, 1)\n",
    "r_score_bins = r_borders\n",
    "\n",
    "r_score = (pd.cut(pre_rfm.day_quantity, \n",
    "                   bins=r_score_bins, \n",
    "                   labels = r_labels, \n",
    "                   right=True, \n",
    "                   include_lowest=True)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим общий датафрэйм, добавив в него рейтинги R, F, M\n",
    "rfm_kmeans = (\n",
    "    pre_rfm\n",
    "        .copy()\n",
    "        .drop('last_order', axis=1)\n",
    "        .assign(recency   = r_score,\n",
    "                frequency = f_score,\n",
    "                monetary  = m_score)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_kmeans.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# среднее recency по кластерам для k-means\n",
    "avg_recency_kmeans = (\n",
    "    rfm_kmeans\n",
    "        .pivot_table(index='recency', values='day_quantity')\n",
    "        .reset_index()\n",
    "        .rename(columns={'recency': 'cluster',\n",
    "                        'day_quantity': 'recency'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# среднее frequency по кластерам для k-means\n",
    "avg_frequency_kmeans = (\n",
    "    rfm_kmeans\n",
    "        .pivot_table(index='frequency', values='purchases_quantity')\n",
    "        .reset_index()\n",
    "        .rename(columns={'frequency': 'cluster',\n",
    "                        'purchases_quantity': 'frequency'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# среднее monetary по кластерам для k-means\n",
    "avg_monetary_kmeans = (\n",
    "    rfm_kmeans\n",
    "        .pivot_table(index='monetary', values='avg_bill')\n",
    "        .reset_index()\n",
    "        .rename(columns={'monetary': 'cluster',\n",
    "                        'avg_bill': 'monetary'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сводная таблица средних значений recency, frequency и monetary по кластерам\n",
    "avg_rfm_kmeans = (\n",
    "    avg_recency_kmeans\n",
    "        .merge(avg_frequency_kmeans, on='cluster')\n",
    "        .merge(avg_monetary_kmeans, on='cluster')\n",
    "        .sort_values('cluster')\n",
    "        .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Средние значения метрик R, F, M для каждого кластера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rfm_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавляем колонку RFM\n",
    "rfm_kmeans['RFM'] = rfm_kmeans.apply(lambda x: str(x.recency) + str(x.frequency) + str(x.monetary), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_kmeans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество пользователей по каждому сегменту\n",
    "customer_count = (\n",
    "            rfm_kmeans\n",
    "                .groupby('RFM', as_index=False)\n",
    "                .agg({'customer': 'count'})\n",
    "                .rename(columns={'customer': 'customer_number'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Следующая таблица отражает сегменты и средние значения recency, frequency и monetary для них, а также количество пользователей по каждому сегменту.\n",
    "\n",
    "rfm_mean = (\n",
    "    rfm_kmeans\n",
    "        .groupby('RFM', as_index=False)\n",
    "        .agg({'day_quantity':      'mean',\n",
    "             'purchases_quantity': 'mean',\n",
    "             'avg_bill':           'mean'})\n",
    "        \n",
    "        # переименуем колонки\n",
    "        .rename(columns={'day_quantity':      'recency_mean',\n",
    "                        'purchases_quantity': 'frequency_mean',\n",
    "                        'avg_bill':           'monetary_mean'})\n",
    "\n",
    "        # добавим количество пользователей по сегментам\n",
    "        .merge(customer_count, on='RFM')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
